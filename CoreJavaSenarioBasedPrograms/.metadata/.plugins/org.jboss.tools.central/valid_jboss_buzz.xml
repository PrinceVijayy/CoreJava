<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Quarkus Tools for IntelliJ 1.14.0 released!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/intellij-quarkus-tools-1.14.0/&#xA;            " /><author><name>Jeff Maury (https://twitter.com/jeffmaury)</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.14.0/</id><updated>2022-11-23T00:00:00Z</updated><published>2022-11-23T00:00:00Z</published><summary type="html">We are very pleased to announce the 1.14.0 release of Quarkus Tools for IntelliJ. This release adds support for CodeActions and Quick Fixes. CodeActions / Quick Fixes When an error is detected on one of your Quarkus project files, it is highlighted in the source editor (for instance when you...</summary><dc:creator>Jeff Maury (https://twitter.com/jeffmaury)</dc:creator><dc:date>2022-11-23T00:00:00Z</dc:date></entry><entry><title type="html">WildFly: How to rollback CLI changes</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-script/wildfly-how-to-rollback-cli-changes/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-script/wildfly-how-to-rollback-cli-changes/</id><updated>2022-11-22T12:50:43Z</updated><content type="html">In this short article we will learn how to rollback WildFly Command Line commands after their execution in order to restore the original XML configuration. WildFly Configuration History The key concept is that WildFly uses the JBOSS_HOME/standalone/configuration/standalone_xml_history folder to store configuration changes. Within this folder you will find the current configuration history, the configuration history ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How I developed a faster Ruby interpreter</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/22/how-i-developed-faster-ruby-interpreter" /><author><name>Vladimir Makarov</name></author><id>d5200cd6-1ab5-4212-8e66-fda53f090fe1</id><updated>2022-11-22T07:00:00Z</updated><published>2022-11-22T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I will describe my efforts to implement a faster interpreter for CRuby, the &lt;a href="https://developers.redhat.com/topics/ruby/all"&gt;Ruby&lt;/a&gt; language interpreter, using a dynamically specialized internal representation (IR). I believe this article will interest developers trying to improve the interpreter performance of dynamic programming languages (e.g., &lt;a href="https://github.com/python/cpython"&gt;CPython&lt;/a&gt; developers).&lt;/p&gt; &lt;p&gt;I will cover the following topics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Existing CRuby interpreter and just-in-time (JIT) compilers for Ruby—MJIT, YJIT, and the MIR-based CRuby JIT compiler at the very early stages of development—along with my motivation to start this project and my initial expectations for the project outcome.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The general approach to performance improvement by specialization and the specializations used in my project to implement a faster CRuby interpreter. I will describe a new dynamically specialized internal representation called SIR, which speeds up the CRuby interpreter in the CRuby virtual machine (VM).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Implementation and current status.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Performance results in comparison with the base interpreter and other CRuby JIT compilers.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;My future plans for this project and the significance of my work for developers.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2 id="the-project-motivation"&gt;The project motivation&lt;/h2&gt; &lt;p&gt;About four years ago, I started a MIR project to address shortcomings of the current CRuby JIT compiler, &lt;a href="https://bugs.ruby-lang.org/projects/ruby/wiki/MJIT"&gt;MJIT&lt;/a&gt;. I started MIR as a lightweight, universal JIT compiler, which could be useful for implementing JIT compilers for Ruby and other programming languages.&lt;/p&gt; &lt;p&gt;MIR is already used for the JIT compilers of several programming languages.&lt;/p&gt; &lt;p&gt;Still, I realize that we can't use the current state of MIR to implement good JIT compilers for dynamic programming languages. Therefore, I've been working on new features. You can read about these features in my previous article, &lt;a href="https://developers.redhat.com/articles/2022/02/16/code-specialization-mir-lightweight-jit-compiler"&gt;Code specialization for the MIR lightweight JIT compiler&lt;/a&gt;. In brief, these features include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A generalized approach of propagation of dynamic properties of source code based on &lt;em&gt;lazy basic block versioning&lt;/em&gt; and generation of specialized code according to the properties&lt;/li&gt; &lt;li&gt;&lt;em&gt;Trace&lt;/em&gt; generation and optimization based on basic block cloning&lt;/li&gt; &lt;li&gt;A &lt;em&gt;metatracing&lt;/em&gt; MIR C compiler, the project's ultimate goal&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Implementation of these features has taken me many years. The recent success of Shopify's &lt;a href="https://github.com/Shopify/yjit"&gt;YJIT&lt;/a&gt; compiler made me rethink my strategy and find a faster way to implement a MIR-based JIT compiler for CRuby.&lt;/p&gt; &lt;p&gt;To implement a decent MIR-based JIT compiler, I decided earlier to develop some features with a design specific to CRuby. I use dynamically specialized instructions for the CRuby VM and generate machine code from the instructions using the MIR compiler in its current state.&lt;/p&gt; &lt;p&gt;Implementing a dynamically specialized IR and an interpreter for it is beneficial, even without a JIT compiler. The resulting design permits the implementation of a faster CRuby without the complexity of JIT compilers and their portability issues.&lt;/p&gt; &lt;p&gt;YJIT is a very efficient JIT compiler, so I have been assessing its features and asking what other features could make a compiler better.&lt;/p&gt; &lt;p&gt;It is essential to start by understanding how much code is covered by a type of optimization. Some optimizations are limited to a single VM instruction.&lt;/p&gt; &lt;p&gt;Most optimizations work within a &lt;em&gt;basic block&lt;/em&gt;, a group of instructions that run sequentially without internal loops or conditional statements. In Ruby, each basic block is enclosed within braces and is often the body of an innermost loop or an &lt;code&gt;if&lt;/code&gt; statement. Optimizations that span more than a single basic block are much more difficult.&lt;/p&gt; &lt;p&gt;For each stack instruction in the VM, YJIT currently generates the best machine code seen in the field. YJIT generates even faster code than the leading open source C compilers, &lt;a href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; and &lt;a href="https://llvm.org/"&gt;LLVM Clang&lt;/a&gt;, for a single VM instruction in the interpreter.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: YJIT was initially written in C but was rewritten in &lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt; to simplify the porting of YJIT to new architectures. The Rust implementation employs abstractions provided currently by the Shopify Ruby team. In my opinion, the best tool to implement a portable YJIT would have been the &lt;a href="https://luajit.org/dynasm.html"&gt;DynASM&lt;/a&gt; C library.&lt;/p&gt; &lt;p&gt;Another compiler technique used by YJIT is &lt;em&gt;basic block versioning&lt;/em&gt;, a powerful technique for dynamically executed languages such as Python and Ruby. I'll describe basic block versioning in the upcoming section, &lt;a href="#specialization-by-lazy-bb-versioning"&gt;Lazy basic block versioning&lt;/a&gt;. The essential idea is that many versions, each with different compiler instructions, exist for each basic block. Some versions are specialized for certain conditions, such as particular data types, and are more efficient than the non-specialized versions when the right conditions hold. The compiler can use the more efficient versions when possible and fall back on less efficient versions under different conditions.&lt;/p&gt; &lt;p&gt;But YJIT's code generation does not span multiple VM instructions. Register transfer language (RTL) is another technique available to compilers, which optimizes code across several stack VM instructions. So I started my new project hoping that, if I implement RTL and use basic block versioning similar to YJIT for some benchmarks, I can match the performance of YJIT even in the interpreter.&lt;/p&gt; &lt;p&gt;I will reveal what I achieved later in this article.&lt;/p&gt; &lt;h3&gt;Code specialization&lt;/h3&gt; &lt;p&gt;I have mentioned "code specialization" several times, but what is specialization? The Merriam-Webster dictionary provides the following definition of the word which is suitable for our purposes: &lt;em&gt;to&lt;/em&gt; &lt;em&gt;design, train, or fit for one particular purpose.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If we generate code optimized for a particular purpose that happens to be a frequent use case, our code will work faster in most cases. Specialization is one common approach to generating faster code. Even static compilers generate specialized code. For instance, they can generate code specialized for a particular processor model, such as a matrix multiplication that fits a particular size of processor cache.&lt;/p&gt; &lt;p&gt;Specialized code also exists in CRuby. For example, the CRuby VM has specialized instructions for calling methods that operate on numbers, the most frequently used data types. The instructions have specialized names such as &lt;code&gt;opt_plus&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The compiler can accomplish code specialization statically or dynamically during program execution. Dynamic specialization adds execution time but is hopefully more efficient because interpreters and JIT compilers have more data during the run to help select a particular case for specialization. That is why JIT compilers usually do more specialization than static compilers.&lt;/p&gt; &lt;p&gt;You can specialize &lt;em&gt;speculatively&lt;/em&gt; even when you cannot guarantee that particular conditions for specialization will always be true. For instance, if a Ruby variable is set to an integer once, you can safely speculate that future assignments to that variable will also be integers. Of course, this assumption will occasionally prove untrue in a dynamically typed language such as Ruby.&lt;/p&gt; &lt;p&gt;Therefore, during speculative specialization, you need guards to check if the initial conditions for specialization still hold true. If these conditions are not true, you switch to less efficient code that does not require those conditions for correct execution. Such code switching is commonly called &lt;em&gt;deoptimization&lt;/em&gt;. Guards are described in the upcoming sections, &lt;a href="#specialization-by-lazy-bb-versioning"&gt;Lazy basic block versioning&lt;/a&gt; and &lt;a href="#profile-based-specialization"&gt;Profile-based specialization&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The more dynamic a programming language, the more specialization and the more speculative specialization you need to achieve performance close to static programming languages.&lt;/p&gt; &lt;h2&gt;8 Optimization techniques&lt;/h2&gt; &lt;p&gt;The following subsections describe the eight techniques I have added to my interpreter and MIR-based JIT compiler.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Dynamically specialized CRuby instructions&lt;/li&gt; &lt;li&gt;RTL code instructions&lt;/li&gt; &lt;li&gt;Hybrid stack/RTL instructions&lt;/li&gt; &lt;li&gt;Type-specialized instructions&lt;/li&gt; &lt;li&gt;Lazy basic block versioning&lt;/li&gt; &lt;li&gt;Profile-based specialization&lt;/li&gt; &lt;li&gt;Specialized iterator instructions&lt;/li&gt; &lt;li&gt;Dynamic flow of specialized instructions&lt;/li&gt; &lt;/ol&gt;&lt;h3 id="dynamically-specialized-cruby-instructions"&gt;1. Dynamically specialized CRuby instructions&lt;/h3&gt; &lt;p&gt;All the specialization I am implementing for CRuby is done &lt;em&gt;dynamically&lt;/em&gt; and &lt;em&gt;lazily&lt;/em&gt;. Currently, I optimize only on the level of a basic block.&lt;/p&gt; &lt;p&gt;I use specialized hybrid stack/RTL instructions. This kind of specialization could be done at compile time, but my interpreter does it lazily as part of a larger technique of generating several different specialized basic blocks. Laziness helps to save memory and time spent on RTL generation. I will explain later why I use hybrid stack/RTL instructions instead of pure RTL.&lt;/p&gt; &lt;p&gt;I also generated &lt;em&gt;type-specialized&lt;/em&gt; instructions. This can be done by &lt;a href="https://arxiv.org/abs/1411.0352"&gt;lazy basic block versioning&lt;/a&gt;, invented by Maxime Chevalier-Boisvert, and used as a major optimization mechanism in YJIT. This optimization is cost-free, and no special guards are needed for checking the value types of instruction input operands. Type specialization is also based on profiling program execution. In this case, the interpreter needs guards to check the types of instruction input operands. Such type of specialization helps to improve cost-free type specialization even further.&lt;/p&gt; &lt;p&gt;Other specializations are based on profile information. Additionally, I included specialized instructions for method calls and accesses to array elements, instance variables, and attributes. The most interesting case is iterator specialization, which I will describe later.&lt;/p&gt; &lt;h3 id="rtl"&gt;2. RTL code instructions&lt;/h3&gt; &lt;p&gt;CRuby uses stack instructions in its VM. Such VM instructions address values implicitly. We can also use VM instructions addressing values explicitly. A set of such instructions is called a register transfer language (RTL).&lt;/p&gt; &lt;p&gt;Here is an example of how the addition of two values is represented by stack instructions and by the RTL instructions generated by my compiler. The number sign (#) is used to start a comment in both languages:&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="541"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Stack instructions&lt;/td&gt; &lt;td&gt;RTL instructions&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;getlocal v1 # push v1&lt;/p&gt; &lt;p&gt;getlocal v2 # push v2&lt;/p&gt; &lt;p&gt;opt_plus # pop v1 and v2 push v1 + v2&lt;/p&gt; &lt;p&gt;setlocal res # pop stack value and assign it to res&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;sir_plusvvv res, v1, v2 # assign v1 + v2 to res&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;As a rule, RTL code contains fewer instructions than stack-based instructions, and as result spends less time in interpreter instruction dispatch code. But RTL sometimes spends more time in operand decoding. More importantly, RTL code results in less memory traffic, because local variables and stack values are addressed directly by RTL instructions. Therefore, stack pushes and pops of local variable values are not as necessary as they are when using stack instructions.&lt;/p&gt; &lt;p&gt;In many cases, CRuby works with values in a stack manner. For example, when pushing values for method calls. So pure RTL has its own disadvantages in such cases and might result in larger code that decodes operands more slowly. Another Ruby-specific problem in RTL lies in implementing fast addressing of Ruby local variables and stack values. Figure 1 shows a typical frame from a Ruby method.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stack2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/stack2.png?itok=gqIjWorB" width="380" height="226" alt="The ep pointer separates the local variables from the stack variables in a Ruby method frame" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The ep pointer separates the local variables from the stack variables in a Ruby method frame. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Addressing values in this kind of stack frame is simple. You just use an index relative to &lt;code&gt;ep&lt;/code&gt; (environment pointer): Negative indices for local variables and positive indices for stack values.&lt;/p&gt; &lt;p&gt;Unfortunately, a method's frame could also look like Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stack3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/stack3.png?itok=RD1WYOnP" width="520" height="226" alt="Another type of frame has the same layout, but inserts an unpredictable distance between the ep pointer and the stack variables" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Another type of frame has the same layout, but inserts an unpredictable distance between the ep pointer and the stack variables. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;For this kind of frame, you need to use &lt;code&gt;ep&lt;/code&gt; for local variables and &lt;code&gt;sp&lt;/code&gt; (stack pointer) for the stack. Other ways of addressing could be used, but they all depend on addressing local and stack variables differently. This means a lot of branches for addressing instruction values.&lt;/p&gt; &lt;p&gt;Still, I used RTL about four years ago, and at that time it gave about a 30% improvement on average on a set of microbenchmarks.&lt;/p&gt; &lt;h3 id="hybrid-stack-rtl-instructions"&gt;3. Hybrid stack/RTL instructions&lt;/h3&gt; &lt;p&gt;Based on my previous experience, I modified my old approach of using RTL and started to use hybrid stack/RTL instructions. These instructions can address some operands implicitly and others explicitly.&lt;/p&gt; &lt;p&gt;RTL instructions are generated only on the level of a basic block, and only lazily on the first execution of a basic block. Figure 3 shows the RTL instructions I added.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/JIT-fig3.png?itok=UoK68-Up" width="544" height="662" alt="RTL instructions naming format." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The names of RTL instructions follow a format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;Here, the suffix (final letter) holds the following meanings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;s&lt;/code&gt;: The value is on the stack.&lt;/li&gt; &lt;li&gt;&lt;code&gt;v&lt;/code&gt;: The value is in a local variable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;i&lt;/code&gt;: The value is an immediate operand.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Some combinations of suffixes are not used. For example, the suffix &lt;code&gt;sss&lt;/code&gt; is not used because an instruction with such a suffix would actually be an existing CRuby stack instruction.&lt;/p&gt; &lt;p&gt;It seems that adding many new VM instructions might result in worse code locality in the interpreter. But in practice, the benefits of reduced dispatching and stack memory traffic outweigh code locality problems. In general code, locality is less important than data locality in modern processors. Just the introduction of hybrid stack/RTL instructions can improve the performance of some benchmarks by 50%.&lt;/p&gt; &lt;h3 id="type-specialized-instructions"&gt;4. Type-specialized instructions&lt;/h3&gt; &lt;p&gt;Integers in the CRuby VM are represented by &lt;a href="https://gmplib.org/"&gt;multi-precision integers&lt;/a&gt; or by &lt;code&gt;fixnum&lt;/code&gt;, a &lt;a href="https://stackoverflow.com/questions/33843393/how-does-ruby-differentiate-value-with-value-and-pointer"&gt;tagged&lt;/a&gt; integer value that fits in one machine word. Floating-point numbers are represented where possible by tagged &lt;a href="https://standards.ieee.org/ieee/754/6210/"&gt;IEEE-754&lt;/a&gt; double values called &lt;code&gt;flonum&lt;/code&gt;, and otherwise by IEEE-754 values in the CRuby heap.&lt;/p&gt; &lt;p&gt;Many CRuby VM instructions are designed to work on primitive data types like a &lt;code&gt;fixnum&lt;/code&gt;. The instructions make a lot of checks before executing the actual operation. For example, &lt;code&gt;opt_plus&lt;/code&gt; checks that input data is &lt;code&gt;fixnum&lt;/code&gt; and that the Ruby &lt;code&gt;+&lt;/code&gt; operator is not redefined for integers. If the checks fail, a general &lt;code&gt;+&lt;/code&gt; method is called.&lt;/p&gt; &lt;p&gt;Type-specialized instructions allowed me to remove the checks and the call code. The optimization resulted in faster and slimmer VM instructions and better interpreter code locality. The new type-specialized instructions are shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/JIT-fig4.png?itok=NqE4UKjv" width="1440" height="414" alt="The type-specialized instructions naming format." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The names of type-specialized instructions follow this format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The prefix takes on the following meanings:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_i&lt;/code&gt;: Denotes instructions specialized for &lt;code&gt;fixnum&lt;/code&gt; (integers).&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_f&lt;/code&gt;: Denotes instructions specialized for &lt;code&gt;flonum&lt;/code&gt; (floating-point numbers).&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_ib&lt;/code&gt;: Used for branch and &lt;code&gt;fixnum&lt;/code&gt; compare instructions.&lt;/p&gt; &lt;p&gt;To guarantee that type-specialized instructions are passed data of the expected types, lazy basic block versioning can be used.&lt;/p&gt; &lt;p&gt;The type-specialized instructions can be also generated from the profile information. In this case, type guards guarantee data of the expected types.&lt;/p&gt; &lt;p&gt;If an exceptional event prevents the remainder of a basic block from executing, the interpreter deoptimizes code by switching to RTL code for the basic block, which is not type-specialized. An example of an exceptional event could be a &lt;code&gt;fixnum&lt;/code&gt; overflow, which requires a multi-precision number result instead of the expected &lt;code&gt;fixnum&lt;/code&gt;. Hybrid stack/RTL and type-specialized instructions are designed not to do any instruction data moves before the switch, which would require pushing variable values on the stack.&lt;/p&gt; &lt;p&gt;The same deoptimization happens if a standard Ruby operator, such as integer &lt;code&gt;+&lt;/code&gt;, is redefined. The deoptimization removes all basic block clones containing type-specialized instructions for this operation, because there is a tiny probability that these basic block clones will be used in the future.&lt;/p&gt; &lt;p&gt;Instructions specialized for &lt;code&gt;flonum&lt;/code&gt; values hardly improve interpreter performance, because most of the instruction execution time is spent tagging and untagging &lt;code&gt;flonum&lt;/code&gt; values, requiring many shifts and logical operations. Therefore, I included the instructions specialized for &lt;code&gt;flonum&lt;/code&gt; values mostly for a future MIR-based JIT compiler, which will remove redundant floating-point number tagging and untagging.&lt;/p&gt; &lt;h3 id="specialization-by-lazy-bb-versioning"&gt;5. Lazy basic block versioning&lt;/h3&gt; &lt;p&gt;This technique is easiest to explain by an example. Take the following &lt;code&gt;while&lt;/code&gt; loop:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ruby"&gt;while i &lt; 100 do i += 1 end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 5 illustrates basic block versioning for this loop.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bbv.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/bbv.png?itok=ty8wm4Oj" width="740" height="860" alt="Basic block versioning creates several alternative ways to step through a basic block." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Basic block versioning creates several alternative ways to step through a basic block. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;Upon its first encounter with the basic block, the interpreter doesn't know the types of values on the stack or in variables. But when we execute &lt;code&gt;BB1&lt;/code&gt; for the first time (see the first diagram), we can easily figure out that the value type of &lt;code&gt;i&lt;/code&gt; became &lt;code&gt;fixnum&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In the top left diagram, the successor of &lt;code&gt;BB1&lt;/code&gt; is &lt;code&gt;BB3&lt;/code&gt; and there is only one version of &lt;code&gt;BB3&lt;/code&gt;, which has no knowledge of variable value types. So we clone &lt;code&gt;BB3&lt;/code&gt; to create &lt;code&gt;BB3v2&lt;/code&gt;, in which the value type of &lt;code&gt;i&lt;/code&gt; is always &lt;code&gt;fixnum&lt;/code&gt;. We make &lt;code&gt;BB3v2&lt;/code&gt; a successor of &lt;code&gt;BB1&lt;/code&gt; (see the second diagram) and start executing &lt;code&gt;BB3v2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;From &lt;code&gt;BB3v2&lt;/code&gt;, since we haven't completed the loop, we go to &lt;code&gt;BB4&lt;/code&gt;. No variable value is changed in &lt;code&gt;BB3v2&lt;/code&gt;. So at the end of &lt;code&gt;BB3v2&lt;/code&gt; the value type of &lt;code&gt;i&lt;/code&gt; is still &lt;code&gt;fixnum&lt;/code&gt;. Therefore, we can create &lt;code&gt;BB4v2&lt;/code&gt; and make it a successor of &lt;code&gt;BB3v2&lt;/code&gt;. Because we know that the value type of &lt;code&gt;i&lt;/code&gt; is &lt;code&gt;fixnum&lt;/code&gt; at the beginning of &lt;code&gt;BB4v2&lt;/code&gt;, we can easily deduce that the type of &lt;code&gt;i&lt;/code&gt; at the end of &lt;code&gt;BB4v2&lt;/code&gt; is also &lt;code&gt;fixnum&lt;/code&gt;. We need a version of &lt;code&gt;BB4v2&lt;/code&gt;'s successor in which the type of &lt;code&gt;i&lt;/code&gt; is &lt;code&gt;fixnum&lt;/code&gt;. Fortunately, such a version already exists: &lt;code&gt;BB3v2&lt;/code&gt;. So we just change the successor of &lt;code&gt;BB4v2&lt;/code&gt; to &lt;code&gt;BB3v2&lt;/code&gt; (see the third diagram) and execute &lt;code&gt;BB3v2&lt;/code&gt; again.&lt;/p&gt; &lt;p&gt;In short, knowing the type of &lt;code&gt;i&lt;/code&gt; permits the interpreter to specialize instructions in &lt;code&gt;BB3v2&lt;/code&gt; and &lt;code&gt;BB4v2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;As you can see, we create basic block versions only when a preceding block is actually executed at run time. This is why such basic block versioning is called &lt;em&gt;lazy&lt;/em&gt;. Usually, we create only a few versions of each basic block. But in pathological cases (which can be demonstrated) a huge number of versions can be created. Therefore, we place a limit on the maximum number of versions for one basic block. When we reach this number, we use an already existing basic block version (usually a version with unknown value types) instead of creating a new one.&lt;/p&gt; &lt;p&gt;Basic block versioning can be also used for other specialization techniques besides type-specialization.&lt;/p&gt; &lt;h3 id="profile-based-specialization"&gt;6. Profile-based specialization&lt;/h3&gt; &lt;p&gt;When the interpreter can't find out the input data types from basic block versioning (e.g., when handling the result of a polymorphic type method call), we insert a &lt;code&gt;sir_inspect_stack_type&lt;/code&gt; or &lt;code&gt;sir_inspect_type&lt;/code&gt; profiling instruction to inspect the types of the stack values or local variables. After the number of executions of a basic block version reaches some threshold, we generate a basic block version with speculatively type-specialized instructions for the data types we found, instead of profiling instructions. Figure 6 shows the format of names of speculatively type-specialized instructions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/speculative-insns-1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/speculative-insns-1.png?itok=zL0iWxC7" width="600" height="261" alt="Speculative instructions" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The names of speculative instructions follow this format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The speculative instructions check the value types of the operands. If the operand doesn't have the expected type, the instruction switches to a non-type specialized version of the basic block.&lt;/p&gt; &lt;p&gt;Figure 7 shows some possible changes made from &lt;code&gt;sir_inspect_type&lt;/code&gt;, used for profiling the types of local variable values.  Instructions &lt;code&gt;sir_inspect_type&lt;/code&gt;, &lt;code&gt;sir_inspect_fixtype&lt;/code&gt;, and &lt;code&gt;sir_inspect_flotype&lt;/code&gt; are self-modified.  Depending on the types of the inspected values at the profiling stage, instead of &lt;code&gt;sir_inspect_type&lt;/code&gt; we will have &lt;code&gt;sir_inspect_fixtype&lt;/code&gt; (if we observed only fixnum types), &lt;code&gt;sir_inspect_flotype&lt;/code&gt; (if we observed only flonum types) or &lt;code&gt;nop&lt;/code&gt; in all other cases.  After the profiling stage we removes all inspect instructions and nops and can generate speculative instructions from non-type-specialized RTL instructions affected by the inspect instructions, e.g. we can generate speculative instruction &lt;code&gt;sir_simultsvv&lt;/code&gt; instead of non-type-specialized RTL instruction &lt;code&gt;sir_multsvv&lt;/code&gt; if we observed that the instruction input values were only of fixnum type.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/inspect-new_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/inspect-new_0.png?itok=v6xmC7rJ" width="600" height="302" alt="Possible changes made from sir_inspect_type, used for profiling types of local variable values" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Possible changes made by sir_inspect_type, used for profiling types of local variable values. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The speculative instructions check data types lazily, only when we do something with the data except data moves. Speculatively type-specialized instructions permit the interpreter to use more of the new non-speculative type-specialized instructions in a basic block version. In these cases, the speculative instructions act as type guards for values used in the subsequent instructions.&lt;/p&gt; &lt;p&gt;Additionally, based on the profile information, the original VM call instructions can be specialized to instructions for C function calls, calls to an iseq (a sequence of VM instructions), or accesses to instance variables.&lt;/p&gt; &lt;h3 id="iterators"&gt;7. Specialized iterator instructions&lt;/h3&gt; &lt;p&gt;Many standard Ruby methods are implemented in C. Some of these methods accept a Ruby block represented by an iseq and behave as iterators.&lt;/p&gt; &lt;p&gt;To execute the Ruby block for each iteration, the C code calls the interpreter. This is a very expensive procedure involving a call to &lt;code&gt;setjmp&lt;/code&gt; (also used to implement CRuby exception handling). We can avoid invoking the interpreter by replacing the method call with specialized iterator instructions:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; sir_iter_start start_func sir_cfunc_send =&gt; Lcont: sir_iter_body Lexit, block_bbv, cond_func sir_iter_cont Lcont, arg_func Lexit:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The iterator instructions are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_start start_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;start_func&lt;/code&gt; checks the receiver type and sets up block arguments.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_body exit_label, block_bbv, cond_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cond_func&lt;/code&gt; finishes the iteration or calls &lt;code&gt;block_bbv&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_cont cont_label, arg_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;arg_func&lt;/code&gt; updates block arguments and permits a goto to &lt;code&gt;cont_label&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Iterator instructions keep their temporary data on the stack. An example of such data is the current array index for an array &lt;code&gt;each&lt;/code&gt; method.&lt;/p&gt; &lt;p&gt;Currently, iterator instructions are implemented only for the &lt;code&gt;fixnum&lt;/code&gt; &lt;code&gt;times&lt;/code&gt; method and for the range and array &lt;code&gt;each&lt;/code&gt; methods. Adding other iterators is easy and straightforward. Usually, you just need to write three very small C functions.&lt;/p&gt; &lt;p&gt;Such specialized iterator instructions can significantly improve performance of Ruby's built-in iterator methods.&lt;/p&gt; &lt;h3 id="dynamic-flow-of-specialized-instructions"&gt;8. Dynamic flow of specialized instructions&lt;/h3&gt; &lt;p&gt;CRuby's front-end, just like the one in CRuby's original implementation, compiles source code into iseq sequences. For each iseq we also create a &lt;em&gt;stub&lt;/em&gt;, an instruction that is executed once and provides the starting point for executing the iseq.&lt;/p&gt; &lt;p&gt;Most executions of basic blocks can rely on assumptions we make about data types during compilation and execution. If we find, during execution, that our speculative assumptions do not hold, we switch back to non-type-specialized hybrid stack/RTL instructions.&lt;/p&gt; &lt;p&gt;For the speculatively type-specialized instructions, the switch can happen when the input value types are not of the expected types. An example situation that violates a speculative assumption for type-specialized instructions is an integer overflow that switches the result type from a &lt;code&gt;fixnum&lt;/code&gt; to a multi-precision number.&lt;/p&gt; &lt;p&gt;Figure 8 shows the flow through a basic block. The downward arrows show the flow that takes place so long as assumptions about data types are valid. When an assumption is invalidated, the path shown by the upward arrows is taken.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sirflow.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sirflow.png?itok=4SR2HhBD" width="600" height="495" alt="The interpreter creates type-specialized instructions and reverts to non-specialized instructions when necessary" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The interpreter creates type-specialized instructions and reverts to non-specialized instructions when necessary. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Execution of a stub creates hybrid stack/RTL instructions for the first basic block of the iseq. At this point, we also generate type-specialized instructions from type information that we know, and profile instructions for values of unknown type. These type-specialized instructions will be the new execution starting point of the iseq during further iterations.&lt;/p&gt; &lt;p&gt;After that, execution continues from the new type-specialized instructions in the basic block. Possible continuations of the basic block at this point are also stubs for successor basic blocks. When the stub of a successor basic block runs, we create hybrid stack/RTL instructions and instructions specialized by type information collected from types in the predecessor basic block. Profiling instructions are also added here. Execution then proceeds with the new basic blocks.&lt;/p&gt; &lt;p&gt;After a specified number of executions of the same basic block, we generate type-specialized instructions and instructions specialized from the collected profile information. And this is now the new starting point of the basic block. The type-specialized and profile-specialized basic block is also a starting point for the MIR-based JIT compiler I am working on. The current MIR-based compiler generates code from the type-specialized and profile-specialized instructions of one basic block. In the future, the compiler will also generate code from the type-specialized and profile-specialized instructions of the entire method.&lt;/p&gt; &lt;h2 id="implementation-and-the-current-status"&gt;Current status of the implementation&lt;/h2&gt; &lt;p&gt;My interpreter and MIR-based JIT compiler that use the specialized IR can be found in &lt;a href="https://github.com/vnmakarov/ruby"&gt;my GitHub repository&lt;/a&gt;. The current state is good only for running the benchmarks I discuss later. Currently, specialized IR generation and execution is implemented in about 3,500 lines of C code. The generator of C code for MIR is about 2,500 lines. The MIR-based JIT compiler needs to build and install the &lt;a href="https://github.com/vnmakarov/mir/tree/bbv"&gt;MIR library&lt;/a&gt;, whose size is about 900KB of machine code. Although his library can be shrunk.&lt;/p&gt; &lt;p&gt;To use the interpreter with the specialized IR, run a program with the &lt;code&gt;--sir&lt;/code&gt; option. There is also an &lt;code&gt;--sir-max-versions=&lt;em&gt;n&lt;/em&gt;&lt;/code&gt; option for setting the maximum number of versions of a basic block.&lt;/p&gt; &lt;p&gt;To use the interpreter with the specialized IR and MIR JIT compiler, run a program with the &lt;code&gt;--mirjit&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;You can also enable the &lt;code&gt;--sir-debug&lt;/code&gt; and &lt;code&gt;--mirjit-debug&lt;/code&gt; debugging options, but please be aware that the debugger output, even for a small Ruby program, will be pretty big.&lt;/p&gt; &lt;h2 id="benchmarking"&gt;Benchmarking the faster interpreter&lt;/h2&gt; &lt;p&gt;I have benchmarked the faster interpreter against the base CRuby interpreter, YJIT, MJIT, and the MIR-based JIT compiler using the following options:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Interpreter with SIR: &lt;code&gt;--sir&lt;/code&gt;&lt;/li&gt; &lt;li&gt;YJIT: &lt;code&gt;--yjit-call-threshold=1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;MJIT: &lt;code&gt;--jit-min-calls=1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;SIR+MIR: &lt;code&gt;--mirjit&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The run time of each benchmark varies from half a minute to a couple of minutes. The run-time options for each JIT compiler are chosen to generate machine code as soon as possible and thus get the best result for that compiler.&lt;/p&gt; &lt;p&gt;I did the benchmarking on an Intel Core i7-9700K with 16GB memory under Linux Fedora Core 32, using my own microbenchmarks which can be found in the &lt;a href="https://github.com/vnmakarov/ruby/tree/sir-mirjit/sir-bench"&gt;sir-bench directory of my repository&lt;/a&gt; and &lt;a href="https://github.com/mame/optcarrot"&gt;Optcarrot&lt;/a&gt;. Each benchmark was run three times and the best result was chosen.&lt;/p&gt; &lt;p&gt;Note that the MIR-based JIT compiler is in the very early stage of development, and I am expecting significant performance improvements in the future.&lt;/p&gt; &lt;h3 id="micro-benchmarks"&gt;Results from microbenchmarks&lt;/h3&gt; &lt;p&gt;Figure 9 shows the wall times for various microbenchmarks.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/JIT-fig9.png?itok=28cJTzcA" width="1172" height="668" alt="A bar graph shows the wall times for various microbenchmarks." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Absolute speeds of four JIT compilers are better or worse on different benchmarks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The following points explain some of the results:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;On Geomean, my new interpreter achieved 109% of the performance of the base CRuby interpreter, but was 6% slower than YJIT.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;RTL with type specialization makes the &lt;code&gt;while&lt;/code&gt; benchmark run faster than YJIT. Using RTL decreases the number of executed instructions per iteration from 8 (stack instructions) to 2 (RTL instructions) and removes 5 CRuby stack accesses.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Iterator specialization permits my interpreter to execute the &lt;code&gt;ntimes&lt;/code&gt;&lt;em&gt; &lt;/em&gt;(nested times) benchmark without leaving and entering the major &lt;code&gt;vm_exec_core&lt;/code&gt; interpreter function. Avoiding that switch results in better code performance. As I wrote earlier, entering the function is very expensive, as it requires a call to the &lt;code&gt;setjmp&lt;/code&gt; C function.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Method calls are where the interpreter's specialization does not work well. YJIT-generated code for the &lt;code&gt;call&lt;/code&gt; benchmark works twice as fast as my interpreter with the specialized IR. YJIT generates code that is already specialized for the call's particular characteristics. For instance, YJIT can reflect the number of arguments and the local variables of the called method. I could add call instructions specialized for these parameters too, but doing so would hugely increase the number of specialized instructions. So I decided not to even try this approach, especially as such specialization will be solved by the MIR-based JIT compiler.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;People often measure only wall time for benchmarks. But CPU use is important too. It reflects how much energy is spent executing the code. CPU time improvements are given in Figure 10.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sir-cpu-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sir-cpu-new.png?itok=j78af0ta" width="576" height="480" alt="CPU time is similar to wall time except for the MJIT compiler." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: CPU time is similar to wall time except for the MJIT compiler. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Differences in CPU usage are comparable to differences in wall time for the microbenchmarks, except for MJIT. MJIT generates machine code using GCC or LLVM in parallel with Ruby program execution. GCC and LLVM do a lot of optimizations and spend a lot of time in them.&lt;/p&gt; &lt;p&gt;YJIT-based and MIR-based JIT compilers do not generate code in parallel. When they decide to JIT-compile some VM instructions, code execution stops until the machine code for these instructions is ready.&lt;/p&gt; &lt;p&gt;Figure 11 shows the maximum resident memory use of my fast interpreter and different JIT compilers, relative to the basic interpreter.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sir-mem-new_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sir-mem-new_1.png?itok=tkalQRld" width="576" height="480" alt="YJIT's maximum memory usage is high" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: YJIT's maximum memory use is high. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;YJIT reserves a big pool of memory for its work. This memory is often not fully used. I assume this YJIT behavior can be improved.&lt;/p&gt; &lt;h3 id="optcarrot"&gt;Optcarrot benchmark results&lt;/h3&gt; &lt;p&gt;Optcarrot, a Nintendo game computer emulator, is a classic benchmark for Ruby. Figure 12 shows the best frame per second (FPS) values when 3,000 frames are generated by Optcarrot.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/carrot-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/carrot-new.png?itok=tjIRQaUN" width="576" height="480" alt="The new interpreter performs better than the basic interpreter on Optcarrot." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: The new interpreter performs better than the basic interpreter on Optcarrot. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The specialized IR shows a 45% improvement over the basic interpreter.&lt;/p&gt; &lt;p&gt;In the optimized version of Optcarrot (Figure 13), a huge method is generated before execution. Basically, the method is a substitute for aggressive method inlining. Because there are a lot fewer method calls, for which specialization in the interpreter is not as good as for JIT compilers, the interpreter with the specialized IR generates the second-best result, right after the MIR-based JIT compiler.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/optcarrot-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/optcarrot-new.png?itok=nRXefVdz" width="576" height="480" alt="The new interpreter performs much better than the basic interpreter on optimized Optcarrot." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: The new interpreter performs much better than the basic interpreter on optimized Optcarrot. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;YJIT behavior is the worst on this benchmark. I did not investigate why YJIT has such low performance in this case. I have heard that the reason might be that YJIT did not implement the &lt;code&gt;opt_case_dispatch&lt;/code&gt; instruction used by the optimized Optcarrot.&lt;/p&gt; &lt;p&gt;Although MJIT produces a decent FPS result, it takes forever to finish the benchmark. Running with MJIT, Ruby terminates only when compilation finishes for all methods currently being compiled by GCC or LLVM. This trait is a specific limitation of the current parallel MJIT engine. Because the huge method requires a lot of time for GCC to compile, the method in this benchmark is actually being executed by the interpreter. The benchmark finishes in a reasonable amount of time, but MJIT is still waiting for the method compilation to finish, even though the generated machine code is never used.&lt;/p&gt; &lt;h2 id="conclusion"&gt;The significance of this prototype for Ruby and Python&lt;/h2&gt; &lt;p&gt;The faster CRuby interpreter described in this article is only a very early prototype. Much needs to be added, and there are still many bugs. I am going to finish the work this year and continue my work on a MIR-based CRuby JIT compiler using the specialized IR. There are still many bugs to fix in the JIT compiler and a lot of work must be done to generate better code.&lt;/p&gt; &lt;p&gt;The specialization described here is useful for developers who already use classical approaches to speed up dynamic programming language interpreters and now want to achieve even better interpreter performance. Currently, I consider the specialized IR and the MIR-based CRuby JIT compiler more as research projects for me than as candidates for production use. The enhancements in the projects demonstrate what can be accomplished with the MIR project.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;Because there is too much technical debt in my different code bases, I probably cannot provide maintenance and adaptation of the code for future CRuby releases. Anyone can use and modify my code for any purpose. I welcome such work. I will provide help where I can, but unfortunately, I cannot commit to this work. However, I am fully committed to maintaining and improving the MIR project.&lt;/p&gt; &lt;p&gt;Currently, CPython developers are working on speeding up their interpreter. Some techniques described here (particularly RTL and basic block versioning) are not used in their project. But these techniques might prove even more profitable than CRuby because CPython uses reference counts for garbage collection. It may be easier to implement the techniques I developed in CPython. Although CPython supports concurrency, it does not have real parallelism as CRuby does.&lt;/p&gt; &lt;p&gt;You can comment below if you have questions. Your feedback is welcome. Developers can also get some practice with hands-on labs in &lt;a href="https://developers.redhat.com/developer-sandbox/activities/learn-kubernetes-using-red-hat-developer-sandbox-openshift"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; for free.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/22/how-i-developed-faster-ruby-interpreter" title="How I developed a faster Ruby interpreter"&gt;How I developed a faster Ruby interpreter&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vladimir Makarov</dc:creator><dc:date>2022-11-22T07:00:00Z</dc:date></entry><entry><title>Redis Job Queue - Reloaded</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/redis-job-queue-reloaded/&#xA;            " /><author><name>Clement Escoffier (https://twitter.com/clementplop)</name></author><id>https://quarkus.io/blog/redis-job-queue-reloaded/</id><updated>2022-11-22T00:00:00Z</updated><published>2022-11-22T00:00:00Z</published><summary type="html">In How to implement a job queue with Redis, we explained how to implement a job queue mechanism with Redis and the new Redis API from Quarkus. The approach explored in that blog post had a significant flaw: if the execution of a job failed, the request was lost and...</summary><dc:creator>Clement Escoffier (https://twitter.com/clementplop)</dc:creator><dc:date>2022-11-22T00:00:00Z</dc:date></entry><entry><title>Why GPUs are essential for AI and high-performance computing</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing" /><author><name>Audrey Reznik, Troy Nelson, Kaitlyn Abdo, Christina Xu</name></author><id>452b1181-0613-4a56-8cf6-16ca8e55a3f5</id><updated>2022-11-21T07:00:00Z</updated><published>2022-11-21T07:00:00Z</published><summary type="html">&lt;p&gt;Graphics processing units (GPU) have become the foundation of artificial intelligence. Machine learning was slow, inaccurate, and inadequate for many of today's applications. The inclusion and utilization of GPUs made a remarkable difference to large neural networks. Deep learning discovered solutions for image and video processing, putting things like autonomous driving or facial recognition into mainstream technology.&lt;/p&gt; &lt;p&gt;The connection between GPUs and OpenShift does not stop at data science. High-performance computing is one of the hottest trends in enterprise tech. Cloud computing creates a seamless process enabling various tasks designated for supercomputers, better than any other computing power you use, saving you time and money.&lt;/p&gt; &lt;h2&gt;How GPUs work&lt;/h2&gt; &lt;p&gt;Let’s back up and make sure we understand how GPUs do what they do.&lt;/p&gt; &lt;p&gt;The term, graphics processing unit, was popularized in 1999 when Nvidia marketed its GeForce 256 with the capabilities of graphics transformation, lighting, and triangle clipping. These are math-heavy computations, which ultimately help render three-dimensional spaces. The engineering is tailored towards these actions, which allows processes to be increasingly optimized and accelerated. Performing millions of computations or using floating point values creates repetition. This is the perfect scenario for tasks to be run in parallel.&lt;/p&gt; &lt;p&gt;GPUs can dominate dozens of CPUs in performance with the help of caching and additional cores. Imagine we are attempting to process high-resolution images. For example, if one CPU takes one minute to process a single image, we would be stuck if we needed to go through nearly a million images for a video. It would take several years to run on a single CPU.&lt;/p&gt; &lt;p&gt;Scaling CPUs will linearly speed up the process. However, even at 100 CPUs, the process would take over a week, not to mention adding quite an expensive bill. A few GPUs, with parallel processing, can solve the problem within a day. We made impossible tasks possible with this hardware.&lt;/p&gt; &lt;h2&gt;The evolution of GPUs&lt;/h2&gt; &lt;p&gt;Eventually, the capabilities of GPUs expanded to include numerous processes, such as artificial intelligence, which often requires running computations on gigabytes of data. Users can easily integrate high-speed computing with simple queries to APIs and coding libraries with the help of complementary software packages for these beasts.&lt;/p&gt; &lt;p&gt;In November 2006, NVIDIA introduced CUDA, a parallel computing platform and programming model. This enables developers to use GPUs efficiently by leveraging the parallel compute engine in NVIDIA’s GPUs and guiding them to partition their complex problems into smaller, more manageable problems where each sub-problem is independent of the other's result.&lt;/p&gt; &lt;p&gt;NVIDIA further spread its roots by partnering with Red Hat OpenShift to adapt CUDA to Kubernetes, allowing customers to develop and deploy applications more efficiently. Prior to this partnership, customers interested in leveraging Kubernetes on top of GPUs had to manually write containers for CUDA and software to integrate Kubernetes with GPUs. This process was time-consuming and prone to errors. Red Hat OpenShift simplified this process by enabling the GPU operator to automatically containerize CUDA and other required software when a customer deploys OpenShift on top of a GPU server. &lt;/p&gt; &lt;p&gt;Red Hat OpenShift Data Science (RHODS) expanded the mission of leveraging and simplifying GPU usage for data science workflows. Now when customers start their Jupyter notebook server on RHODS, they have the option to customize the number of GPUs required for their workflow and select Pytorch and TensorFlow GPU-enabled notebook images. You may be able to select 1 or more GPUs, depending on the GPU machine pool added to your cluster. Customers have the power to use GPUs in their data mining and model processing tasks. &lt;/p&gt; &lt;h2&gt;GPUs in RHODS&lt;/h2&gt; &lt;p&gt;Interested in hearing more about using GPUs in RHODS?  Then check out our new learning path &lt;em&gt;Configure a Jupyter notebook to use GPUs for AI/ML modeling&lt;/em&gt;, which can be found under the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started"&gt;Getting Started section&lt;/a&gt; in our &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/overview"&gt;RHODS public sandbox&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing" title="Why GPUs are essential for AI and high-performance computing"&gt;Why GPUs are essential for AI and high-performance computing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Audrey Reznik, Troy Nelson, Kaitlyn Abdo, Christina Xu</dc:creator><dc:date>2022-11-21T07:00:00Z</dc:date></entry><entry><title>.NET, Go, Kamelets, and more: Top articles from November 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/18/net-go-kamelets-and-more-top-articles-november-2022" /><author><name>Heiker Medina</name></author><id>8a3c16cc-51fd-42d2-bf3d-741065b76de7</id><updated>2022-11-18T19:37:10Z</updated><published>2022-11-18T19:37:10Z</published><summary type="html">&lt;p&gt;Whether you'll soon be signing off for a fall November break or working through the end of the month, take a moment to check out Red Hat Developer's latest top-performing articles. We've highlighted the tutorial guides and announcements that our developer community has engaged with the most.&lt;/p&gt; &lt;h2&gt;Announcements&lt;/h2&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/net-7-now-available-rhel-and-openshift"&gt;.NET 7 now available for RHEL and OpenShift&lt;/a&gt;:&lt;/strong&gt; This short overview from John Clingan discusses what you need to know about &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt; 7. The .NET 7 release is now available, targeting &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8.7, RHEL 9.1, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/04/intros-deep-dives-and-announcements-our-best-october-2022"&gt;Intros, deep dives, and announcements: Our best of October 2022&lt;/a&gt;:&lt;/strong&gt; Here are some highlights from Red Hat Developer for October 2022, organized by product announcements, topic roundups, learning guides, and advanced deep dives&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Tutorials&lt;/h2&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/14/3-ways-embed-commit-hash-go-programs"&gt;3 ways to embed a commit hash in Go programs&lt;/a&gt;:&lt;/strong&gt; When you need to view the source code of an older version of your software, it is handy to have the history feature turned on. Panagiotis Georgiadis's guide explains how to see what your &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; program looked like at various points in time, so you can debug issues that may have been introduced at specific points.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/09/how-static-application-security-testing-improves-software-security"&gt;How static application security testing improves software security&lt;/a&gt;:&lt;/strong&gt; Join Florencio Cano Gabarda as he explains why static application security testing (SAST) is an effective tool for improving the security of your application. Developers can use SAST to identify potential security problems in the source code for an application, its bytecode, or its binary code. Many SAST tools are mature and widely used by software developers.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/introduction-debug-events-learn-how-use-breakpoints"&gt;An introduction to debug events: Learn how to use breakpoints&lt;/a&gt;:&lt;/strong&gt; This article kicks off a series about GDB's debugging capabilities. Keith Seitz will be covering the commands, convenience variables, and functions that will aid you in stopping GDB at the right place at the right time.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/01/how-kamelets-simplify-camel-integrations-kubernetes"&gt;How Kamelets simplify Camel integrations on Kubernetes&lt;/a&gt;:&lt;/strong&gt; Modern applications are often made of a collection of several smaller applications, or subsystems. This article explains how Apache Camel and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; make it easy to integrate such services through Kamelets.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/07/build-reactive-apps-kubernetes-using-camel-k"&gt;Build reactive apps on Kubernetes using Camel K&lt;/a&gt;:&lt;/strong&gt; In this article, Sumit Mukherjee will explain how &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Apache Camel K&lt;/a&gt; can make it easier to develop reactive applications on Kubernetes by integrating data sources, &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; brokers, and Knative for event management.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Node.js Reference Architecture, Part 10: Accessibility&lt;/a&gt;:&lt;/strong&gt; Michael Dawson guides you through the importance of integrating accessibility within your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications. Making your applications accessible to disabled users is good business and often required by law. As a Node.js developer, you need to understand the issues around accessibility so that you can build truly accessible components into the applications you build.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/set-openshift-cluster-deploy-application-odo-cli"&gt;Set up an OpenShift cluster to deploy an application in odo CLI&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/implement-restapi-application-mongodb-using-sbo"&gt;Implement a Rest API application with MongoDB using SBO&lt;/a&gt;:&lt;/strong&gt; In this two-part series, Francesco Ilario shows you how to use &lt;code&gt;odo&lt;/code&gt; to create an application and a database service, bind the application to the database using the Service Binding Operator, and get access to the application's REST API. You'll deploy a REST API application and bind it to a MongoDB using &lt;code&gt;odo&lt;/code&gt; and the Service Binding Operator. The series shows you how easy it is to create an instance of a database and a binding (connection) between the application and that database.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;November 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer this month:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing"&gt;Why GPUs are essential for AI and high-performance computing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications"&gt;Modernize at scale with the new migration toolkit for applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/17/new-features-openmp-51-and-openmp-52"&gt;New features in OpenMP 5.1 and OpenMP 5.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300"&gt;Benchmarking improved conntrack performance in OvS 3.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/16/whats-new-red-hat-enterprise-linux-91"&gt;What's new in Red Hat Enterprise Linux 9.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/16/openshift-essential-containerized-applications"&gt;Why OpenShift is essential for containerized applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/15/knative-broker-enhances-kafka-openshift-serverless"&gt;How Knative broker GA enhances Kafka on OpenShift Serverless&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/15/how-categorize-c-programs-behavior"&gt;How to categorize C programs by behavior&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/14/3-ways-embed-commit-hash-go-programs"&gt;3 ways to embed a commit hash in Go programs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/implement-restapi-application-mongodb-using-sbo"&gt;Implement a Rest API application with MongoDB using SBO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/set-openshift-cluster-deploy-application-odo-cli"&gt;Set up an OpenShift cluster to deploy an application in odo CLI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/09/how-static-application-security-testing-improves-software-security"&gt;How static application security testing improves software security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/net-7-now-available-rhel-and-openshift"&gt;.NET 7 now available for RHEL and OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/visual-guide-deploying-jboss-eap-aws"&gt;A visual guide to deploying JBoss EAP on AWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/introduction-debug-events-learn-how-use-breakpoints"&gt;An introduction to debug events: Learn how to use breakpoints&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/07/build-reactive-apps-kubernetes-using-camel-k"&gt;Build reactive apps on Kubernetes using Camel K&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Node.js Reference Architecture, Part 10: Accessibility&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/how-next-10-project-supports-future-nodejs"&gt;How the Next-10 project supports the future of Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/01/how-kamelets-simplify-camel-integrations-kubernetes"&gt;How Kamelets simplify Camel integrations on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/10/31/best-practices-application-shutdown-openssl"&gt;Best practices for application shutdown with OpenSSL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/18/net-go-kamelets-and-more-top-articles-november-2022" title=".NET, Go, Kamelets, and more: Top articles from November 2022"&gt;.NET, Go, Kamelets, and more: Top articles from November 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-11-18T19:37:10Z</dc:date></entry><entry><title>Modernize at scale with the new migration toolkit for applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" /><author><name>Yashwanth Maheshwaram</name></author><id>f8db9619-4659-4a29-819c-b34658a17249</id><updated>2022-11-18T07:00:00Z</updated><published>2022-11-18T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;migration toolkit for applications &lt;/a&gt;from Red Hat equips developers with tools to assess, prioritize, and modernize &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; applications across the hybrid cloud on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Version 6 of the toolkit is now generally available—and it's a major step up from the previous version, offering new capabilities to accelerate large-scale application modernization efforts. Read on to explore the new toolkit and see a demo.&lt;/p&gt; &lt;p&gt;Based on the open source &lt;a href="https://www.konveyor.io/"&gt;Konveyor&lt;/a&gt; project, the migration toolkit for applications provides insights and alignment for project leads and migration teams as they move to Red Hat OpenShift for a single application or a portfolio of applications. These insights are a huge boost for developers and organizations looking to modernize and migrate from their legacy platforms to the cloud.&lt;/p&gt; &lt;h2&gt;What's new in migration toolkit for applications&lt;/h2&gt; &lt;p&gt;Version 6 of the migration toolkit includes the following tools for your app modernization:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;New application inventory and assessment modules&lt;/strong&gt; that assist organizations in managing, classifying, and tagging their applications while assessing application suitability for deployment in containers, including flagging potential risks for migration strategies.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Full integration with source code and binary repositories&lt;/strong&gt; to automate the retrieval of applications for analysis along with proxy integration, including HTTP and HTTPS proxy configuration managed in the user interface.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Improved analysis capabilities&lt;/strong&gt; with new analysis modes, including source and dependency modes that parse repositories to gather dependencies and add them to the overall scope of the analysis. There is also a simplified user experience to configure the analysis scope, including open source libraries.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Enhanced role-based access control (RBAC)&lt;/strong&gt; powered by &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt;, defining three new differentiated personas with different permissions to suit the needs of each user—administrator, architect, and migrator—including credentials management for multiple credential types.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;An administrator perspective&lt;/strong&gt; to provide tool-wide configuration management for administrators.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Included with a Red Hat OpenShift subscription, the migration toolkit for applications helps reduce complexity and risks to accelerate the modernization of your non-cloud-enabled applications. &lt;/p&gt; &lt;h2&gt;&lt;span&gt; &lt;/span&gt;Watch a demo&lt;/h2&gt; &lt;p&gt;To see version 6 of the migration toolkit in action, check out the following demo.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Supported modernization paths&lt;/h2&gt; &lt;p&gt;Wondering if Red Hat's migration toolkit for applications will support your modernization paths? Our team has compiled a list of the most commonly used migration paths, along with the most common use cases for the toolkit. See the &lt;a href="https://developers.redhat.com/products/mta/use-cases"&gt;migration toolkit for applications page&lt;/a&gt; for the full list of supported migration paths.&lt;/p&gt; &lt;h2&gt;Get started with the migration toolkit on OpenShift&lt;/h2&gt; &lt;p&gt;You can install the migration toolkit for applications on OpenShift 4.9+ with an Operator. See &lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Get started with the migration toolkit for applications&lt;/a&gt; for instructions.&lt;/p&gt; &lt;p&gt;If you would like to try the command-line interface (CLI), there is also a &lt;a href="https://developers.redhat.com/products/mta/download"&gt;download&lt;/a&gt; option available.&lt;/p&gt; &lt;h2&gt; &lt;/h2&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" title="Modernize at scale with the new migration toolkit for applications"&gt;Modernize at scale with the new migration toolkit for applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2022-11-18T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.0.0.Alpha1 released - First iteration of our Jakarta EE 10 stream</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-0-alpha1-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-alpha1-released/</id><updated>2022-11-18T00:00:00Z</updated><published>2022-11-18T00:00:00Z</published><summary type="html">Last week, Max Andersen explained our plans for Quarkus 3 and Jakarta EE 10. I published yesterday a more detailed blog post explaining how we are building the Quarkus 3 stream. It is now time to announce Quarkus 3.0.0.Alpha1 which is the first iteration of our Quarkus 3 stream. A...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-11-18T00:00:00Z</dc:date></entry><entry><title>Benchmarking improved conntrack performance in OvS 3.0.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300" /><author><name>Robin Jarry</name></author><id>97deb5b4-748c-4ad1-93c3-722f5f8e2381</id><updated>2022-11-17T07:00:00Z</updated><published>2022-11-17T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt; (OvS), an open source tool for creating virtual Layer 2 networks, relies in some use cases on connection tracking. The recent 3.0.0 release of OvS included &lt;a href="https://github.com/openvswitch/ovs/compare/cfba95158518...b159525903d1"&gt;this patch series&lt;/a&gt; to improve multithread scalability, which makes connection tracking more efficient when OvS is run on multiple CPUs. This article shows how to measure the performance of connection tracking with OvS.&lt;/p&gt; &lt;h2&gt;What is connection tracking and why is it critical?&lt;/h2&gt; &lt;p&gt;Connection tracking, or &lt;em&gt;conntrack,&lt;/em&gt; maintains an internal table of logical network connections (also called &lt;em&gt;flows&lt;/em&gt;). The table identifies all packets that make up each flow so that they can be handled consistently.&lt;/p&gt; &lt;p&gt;Conntrack is a requirement for network address translation (NAT)—in IP address masquerading, for example (described in detail in &lt;a href="https://www.rfc-editor.org/rfc/rfc3022"&gt;RFC 3022&lt;/a&gt;). Conntrack is also required for stateful firewalls, load balancers, intrusion detection and prevention systems, and deep packet inspection. More specifically, OvS conntrack rules are used to isolate different OpenStack virtual networks (aka &lt;em&gt;security groups&lt;/em&gt;).&lt;/p&gt; &lt;p&gt;Connection tracking is usually implemented by storing known connection entries in a table, indexed by a bidirectional 5-tuple consisting of a protocol, source address, destination address, source port, and destination port. Each entry also has a state as seen from the connection tracking system. The state (&lt;em&gt;new&lt;/em&gt;, &lt;em&gt;established&lt;/em&gt;, &lt;em&gt;closed&lt;/em&gt;, etc.) is updated every time a packet matching its 5-tuple is processed. If a received packet does not match any existing conntrack entry, a new one is created and inserted into the table.&lt;/p&gt; &lt;h2&gt;Performance aspects&lt;/h2&gt; &lt;p&gt;There are two aspects to consider when measuring conntrack performance.&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;em&gt;How many new connections can be handled per second?&lt;/em&gt; This question depends on the following details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;What is the cost of looking up an existing connection entry for each received packet?&lt;/li&gt; &lt;li&gt;Can multiple threads insert and destroy conntrack entries concurrently?&lt;/li&gt; &lt;li&gt;What is the cost of creating a conntrack entry for a new connection?&lt;/li&gt; &lt;li&gt;How many packets are exchanged per connection?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;em&gt;How many concurrent connections can the system support?&lt;/em&gt; This question depends on the following details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;What is the size of the conntrack table?&lt;/li&gt; &lt;li&gt;What is the duration of each individual connection?&lt;/li&gt; &lt;li&gt;After a connection has been closed, how long does the conntrack entry linger in the table until it is expunged to make room for new connections? What if the connection is not closed but no longer exchanges traffic (because the client or server crashed or disconnected)?&lt;/li&gt; &lt;li&gt;What happens when the conntrack table is full?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These two aspects of performance are somewhat connected, because even a low rate of very long new connections causes the conntrack table to fill up eventually.&lt;/p&gt; &lt;p&gt;In order to properly size the connection tracking table, one needs to know the average number of new connections per second and their average duration. Testing also requires tuning the timeout values of the conntrack engine.&lt;/p&gt; &lt;h2 id="benchmarking-process"&gt;Benchmarking process&lt;/h2&gt; &lt;p&gt;To take the measurements necessary to answer the questions in the previous section, you need a way to simulate clients and servers. Such a system must specify how many clients and servers to test, how many connections per second they are creating, how long the connections are, and how much data is exchanged in each connection.&lt;/p&gt; &lt;p&gt;A few commercial traffic generators have these capabilities, more or less refined. This article describes how to carry out the simulation with &lt;a href="https://trex-tgn.cisco.com/"&gt;TRex&lt;/a&gt;—an open source traffic generator based on the &lt;a href="https://www.dpdk.org/"&gt;Data Plane Development Kit&lt;/a&gt; (DPDK).&lt;/p&gt; &lt;p&gt;TRex has multiple modes of operation. This article uses the &lt;a href="https://trex-tgn.cisco.com/trex/doc/trex_astf.html"&gt;advanced stateful&lt;/a&gt; (ASTF) mode, which allows TRex to simulate TCP and UDP endpoints. &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;I have tailored a script using the TRex Python API&lt;/a&gt; to perform benchmarks in a manner like &lt;a href="https://www.rfc-editor.org/rfc/rfc2544.html"&gt;RFC 2544&lt;/a&gt;, but focusing on how many new connections can be created per second.&lt;/p&gt; &lt;p&gt;Basically, this script connects to a running TRex server started in ASTF mode and creates TCP and UDP connection profiles. These profiles are state machines representing clients and servers with dynamic IP addresses and ports. You can define the number of data exchanges and their sizes, add some arbitrary wait time to simulate network latency, etc. TRex takes care of translating your specifications into real traffic.&lt;/p&gt; &lt;p&gt;Here is a stripped down example, in Python, of a TCP connection profile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;client = ASTFProgram(stream=True) server = ASTFProgram(stream=True) for _ in range(num_messages): client.send(message_size * b"x") server.recv(message_size) if server_wait &gt; 0: server.delay(server_wait * 1000) # trex wants microseconds server.send(message_size * b"y") client.recv(message_size) tcp_profile = ASTFTemplate( client_template=ASTFTCPClientTemplate( program=client, port=8080, cps=99, # base value which is changed during the binary search cont=True, ), server_template=ASTFTCPServerTemplate( program=server, assoc=ASTFAssociationRule(port=8080) ), )&lt;/code&gt;&lt;/pre&gt; &lt;h2 id="setup"&gt;Setup&lt;/h2&gt; &lt;p&gt;The device under test (DUT) runs the &lt;code&gt;ovs-vswitchd&lt;/code&gt; Open vSwitch daemon with the user-space DPDK datapath. The setup can be used to benchmark any connection-tracking device. This procedure is overly simple and does not represent an actual production workload. However, it allows you to stress the connection tracking code path without bothering about the external details.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the relationship between the DUT and the traffic generator, which the test creates. Traffic simulating the clients travels from &lt;code&gt;port0&lt;/code&gt; to &lt;code&gt;port1&lt;/code&gt; on traffic generator through the DUT. Server traffic travels from &lt;code&gt;port1&lt;/code&gt; to &lt;code&gt;port0&lt;/code&gt; on the traffic generator. Conntrack flows are programmed on &lt;code&gt;br0&lt;/code&gt; to only allow &lt;em&gt;new&lt;/em&gt; connections to be established from &lt;code&gt;port0&lt;/code&gt; to &lt;code&gt;port1&lt;/code&gt; (from "clients" to "servers") and also allow the reply packets on &lt;em&gt;established&lt;/em&gt; connections from &lt;code&gt;port1&lt;/code&gt; to &lt;code&gt;port0&lt;/code&gt; (from "servers" to "clients") to go through.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_20.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_20.png?itok=aVJPnB5M" width="600" height="248" alt="Network topology diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Network topology. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3 id="base-system"&gt;Base system&lt;/h3&gt; &lt;p&gt;Both the OvS user-space datapath and TRex use DPDK. The settings shown in this section are common to both machines.&lt;/p&gt; &lt;p&gt;DPDK requires compatible network interfaces. The example in this article runs on the last two ports of an Intel X710 PCI network interface. The following commands show the hardware in use:&lt;/p&gt; &lt;pre&gt; [root@* ~]# lscpu | grep -e "^Model name:" -e "^NUMA" -e MHz NUMA node(s): 1 Model name: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz CPU MHz: 2700.087 NUMA node0 CPU(s): 0-23 [root@* ~]# grep ^MemTotal /proc/meminfo MemTotal: 65373528 kB [root@* ~]# lspci | grep X710 | tail -n2 18:00.2 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 02) 18:00.3 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 02) &lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To make things simpler, all commands in this article are executed as the &lt;code&gt;root&lt;/code&gt; user.&lt;/p&gt; &lt;p&gt;The CPUs used by TRex and OvS need to be isolated in order to minimize disturbance from the other tasks running on Linux. Therefore, the following commands isolate CPUs from the NUMA node where the PCI NIC is connected. CPUs 0 and 12 are left to Linux:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y tuned tuned-profiles-cpu-partitioning cat &gt; /etc/tuned/cpu-partitioning-variables.conf &lt;&lt;EOF isolated_cores=1-11,13-23 no_balance_cores=1-11,13-23 EOF tuned-adm profile cpu-partitioning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, DPDK applications require huge pages. It is best to allocate them at boot time to ensure that they are all mapped to contiguous chunks of memory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &gt;&gt; /etc/default/grub &lt;&lt;EOF GRUB_CMDLINE_LINUX="\$GRUB_CMDLINE_LINUX intel_iommu=on iommu=pt" GRUB_CMDLINE_LINUX="\$GRUB_CMDLINE_LINUX hugepagesz=1G hugepages=32" EOF grub2-mkconfig -o /etc/grub2.cfg dnf install -y driverctl driverctl set-override 0000:18:00.2 vfio-pci driverctl set-override 0000:18:00.3 vfio-pci # reboot is required to apply isolcpus and allocate hugepages on boot systemctl reboot &lt;/code&gt;&lt;/pre&gt; &lt;h3 id="traffic-generator"&gt;TRex and the traffic generator&lt;/h3&gt; &lt;p&gt;TRex needs to be compiled from source. The following commands download and build the program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y python3 git numactl-devel zlib-devel gcc-c++ gcc git clone https://github.com/cisco-system-traffic-generator/trex-core ~/trex cd ~/trex/linux_dpdk ./b configure taskset 0xffffffffff ./b build &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We use the following configuration in &lt;code&gt;/etc/trex_cfg.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- version: 2 interfaces: - "18:00.2" - "18:00.3" rx_desc: 4096 tx_desc: 4096 port_info: - dest_mac: "04:3f:72:f2:8f:33" src_mac: "04:3f:72:f2:8f:32" - dest_mac: "04:3f:72:f2:8f:32" src_mac: "04:3f:72:f2:8f:33" c: 22 memory: mbuf_64: 30000 mbuf_128: 500000 mbuf_256: 30717 mbuf_512: 30720 mbuf_1024: 30720 mbuf_2048: 4096 platform: master_thread_id: 0 latency_thread_id: 12 dual_if: - socket: 0 threads: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, we can start TRex:&lt;/p&gt; &lt;pre&gt; cd ~/trex/scripts ./t-rex-64 -i --astf &lt;/pre&gt; &lt;p&gt;The TRex daemon runs in the foreground. The &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;&lt;code&gt;cps_ndr.py&lt;/code&gt;&lt;/a&gt; script connects to the daemon via the JSON-RPC API in a separate terminal.&lt;/p&gt; &lt;h3 id="device-under-test"&gt;The device under test&lt;/h3&gt; &lt;p&gt;First, let's compile and install DPDK:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y git meson ninja-build gcc python3-pyelftools git clone -b v21.11 https://github.com/DPDK/dpdk ~/dpdk cd ~/dpdk meson build taskset 0xffffff ninja -C ~/dpdk/build install &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then compile and install OVS. In the following console excerpt, I explicitly check out version 2.17.2. Version 3.0.0 will be recompiled before running all tests again:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y gcc-g++ make libtool autoconf automake git clone -b v2.17.2 https://github.com/openvswitch/ovs ~/ovs cd ~/ovs ./boot.sh PKG_CONFIG_PATH="/usr/local/lib64/pkgconfig" ./configure --with-dpdk=static taskset 0xffffff make install -j24 /usr/local/share/openvswitch/scripts/ovs-ctl start &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here I enable the DPDK user-space datapath and configure a bridge with two ports. For now, there is only one receive (RX) queue per port, and one CPU is assigned to poll them. I will increase these parameters along the way.&lt;/p&gt; &lt;p&gt;I set the conntrack table size to a relatively large value (5 million entries) to reduce the risk of it getting full during tests. Also, I configure the various timeout policies to match the traffic profiles I am about to send. These aggressive timeouts help prevent the table from getting full. The default timeout values are very conservative—they're too long to achieve high numbers of connections per second without filling the conntrack table:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:dpdk-init=true ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" /usr/local/share/openvswitch/scripts/ovs-ctl restart ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev ovs-vsctl add-port br0 port0 -- \ set interface port0 type=dpdk options:dpdk-devargs=0000:18:00.2 ovs-vsctl add-port br0 port1 -- \ set Interface port1 type=dpdk options:dpdk-devargs=0000:18:00.3 ovs-appctl dpctl/ct-set-maxconns 5000000 # creating an empty datapath record is required to add a zone timeout policy ovs-vsctl -- --id=@m create Datapath datapath_version=0 -- \ set Open_vSwitch . datapaths:"netdev"=@m ovs-vsctl add-zone-tp netdev zone=0 \ udp_first=1 udp_single=1 udp_multiple=30 tcp_syn_sent=1 \ tcp_syn_recv=1 tcp_fin_wait=1 tcp_time_wait=1 tcp_close=1 \ tcp_established=30 cat &gt; ~/ct-flows.txt &lt;&lt; EOF priority=1 ip ct_state=-trk actions=ct(table=0) priority=1 ip ct_state=+trk+new in_port=port0 actions=ct(commit),normal priority=1 ip ct_state=+trk+est actions=normal priority=0 actions=drop EOF &lt;/code&gt;&lt;/pre&gt; &lt;h2 id="test-procedure"&gt;Test procedure&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;&lt;code&gt;cps_ndr.py&lt;/code&gt;&lt;/a&gt; script that I have written has multiple parameters to control the nature of the generated connections:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Ratio of TCP connections to UDP connections&lt;/li&gt; &lt;li&gt;Number of data messages (request + response) exchanged per connection (excluding protocol overhead)&lt;/li&gt; &lt;li&gt;Size of data messages in bytes (to emulate the TCP maximum segment size)&lt;/li&gt; &lt;li&gt;Time in milliseconds that the simulated servers wait before sending a response to a request&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the context of this benchmark, I intentionally keep the size of data messages fixed to 20 bytes, to avoid being limited by the 10Gbit bandwidth.&lt;/p&gt; &lt;p&gt;I run two types of test: One with short-lived connections and the other with long-lived connections. Both the short-lived and long-lived connection profiles are tested against OVS versions 2.17.2 and 3.0.0. Different configurations are tested to check whether performance scales with the number of CPUs and receive queues.&lt;/p&gt; &lt;h3 id="short-lived-connections"&gt;Short-lived connections&lt;/h3&gt; &lt;p&gt;The parameters of this test consist of sending 40 data bytes per connection (1 request + 1 reply of 20 bytes each), with no wait by the server before sending the replies. These parameters stress the conntrack &lt;strong&gt;creation and destruction&lt;/strong&gt; code path.&lt;/p&gt; &lt;p&gt;An example run follows:&lt;/p&gt; &lt;pre&gt; [root@tgen scripts]# ./cps_ndr.py --sample-time 30 --max-iterations 8 \ &gt; --error-threshold 0.02 --udp-percent 1 --num-messages 1 \ &gt; --message-size 20 --server-wait 0 -m 1k -M 100k ... iteration #1: lower=1.0K current=50.5K upper=100K ▼▼▼ Flows: active 26.8K (50.1K/s) TX: 215Mb/s (345Kp/s) RX: 215Mb/s (345Kp/s) Size: ~4.5B err dropped: 1.6K pkts (1.6K/s) ~ 0.4746% ... iteration #2: lower=1.0K current=25.8K upper=50.5K ▲▲▲ Flows: active 12.9K (25.7K/s) TX: 112Mb/s (179Kp/s) RX: 112Mb/s (179Kp/s) Size: ~4.5B ... iteration #3: lower=25.8K current=38.1K upper=50.5K ▲▲▲ Flows: active 19.1K (38.1K/s) TX: 166Mb/s (266Kp/s) RX: 166Mb/s (266Kp/s) Size: ~4.5B ... iteration #4: lower=38.1K current=44.3K upper=50.5K ▼▼▼ Flows: active 22.2K (44.2K/s) TX: 192Mb/s (307Kp/s) RX: 191Mb/s (307Kp/s) Size: ~4.5B err dropped: 1.3K pkts (125/s) ~ 0.0408% ... iteration #5: lower=38.1K current=41.2K upper=44.3K ▲▲▲ Flows: active 20.7K (41.2K/s) TX: 178Mb/s (286Kp/s) RX: 178Mb/s (286Kp/s) Size: ~4.5B ... iteration #6: lower=41.2K current=42.8K upper=44.3K ▼▼▼ Flows: active 21.5K (42.6K/s) TX: 185Mb/s (296Kp/s) RX: 185Mb/s (296Kp/s) Size: ~4.5B err dropped: 994 pkts (99/s) ~ 0.0335% ... iteration #7: lower=41.2K current=42.0K upper=42.8K ▼▼▼ Flows: active 21.0K (41.8K/s) TX: 181Mb/s (290Kp/s) RX: 181Mb/s (290Kp/s) Size: ~4.5B err dropped: 877 pkts (87/s) ~ 0.0301% ... iteration #8: lower=41.2K current=41.6K upper=42.0K ▲▲▲ Flows: active 20.9K (41.4K/s) TX: 180Mb/s (289Kp/s) RX: 180Mb/s (289Kp/s) Size: ~4.5B &lt;/pre&gt; &lt;h3 id="long-lived-connections"&gt;Long-lived connections&lt;/h3&gt; &lt;p&gt;The parameters of this test consist of sending 20K data bytes per connection (500 requests + 500 replies of 20 bytes each) over 25 seconds. These parameters stress the conntrack &lt;strong&gt;lookup&lt;/strong&gt; code path.&lt;/p&gt; &lt;p&gt;An example run follows:&lt;/p&gt; &lt;pre&gt; [root@tgen scripts]# ./cps_ndr.py --sample-time 120 --max-iterations 8 \ &gt; --error-threshold 0.02 --udp-percent 1 --num-messages 500 \ &gt; --message-size 20 --server-wait 50 -m 500 -M 2k ... iteration #1: lower=500 current=1.2K upper=2.0K ▼▼▼ Flows: active 48.5K (1.2K/s) TX: 991Mb/s (1.5Mp/s) RX: 940Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 1.8M pkts (30.6K/s) ~ 2.4615% ... iteration #2: lower=500 current=875 upper=1.2K ▲▲▲ Flows: active 22.5K (871/s) TX: 871Mb/s (1.3Mp/s) RX: 871Mb/s (1.3Mp/s) Size: ~13.3B ... iteration #3: lower=875 current=1.1K upper=1.2K ▼▼▼ Flows: active 33.8K (1.1K/s) TX: 967Mb/s (1.4Mp/s) RX: 950Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 621K pkts (10.3K/s) ~ 0.7174% ... iteration #4: lower=875 current=968 upper=1.1K ▲▲▲ Flows: active 24.9K (965/s) TX: 961Mb/s (1.4Mp/s) RX: 962Mb/s (1.4Mp/s) Size: ~13.3B ... iteration #5: lower=968 current=1.0K upper=1.1K ▼▼▼ Flows: active 29.8K (1.0K/s) TX: 965Mb/s (1.4Mp/s) RX: 957Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 334K pkts (5.6K/s) ~ 0.3830% ... iteration #6: lower=968 current=992 upper=1.0K ▼▼▼ Flows: active 25.5K (989/s) TX: 964Mb/s (1.4Mp/s) RX: 964Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 460 pkts (460/s) ~ 0.0314% ... iteration #7: lower=968 current=980 upper=992 ▼▼▼ Flows: active 25.3K (977/s) TX: 962Mb/s (1.4Mp/s) RX: 962Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 397 pkts (397/s) ~ 0.0272% ... iteration #8: lower=968 current=974 upper=980 ▲▲▲ Flows: active 25.1K (971/s) TX: 969Mb/s (1.5Mp/s) RX: 969Mb/s (1.5Mp/s) Size: ~13.3B &lt;/pre&gt; &lt;h2 id="results"&gt;Performance statistics&lt;/h2&gt; &lt;p&gt;This section presents results of runs with varying numbers of CPUs and queues on my test system. The numbers that I measured should be taken with a grain of salt. Connection tracking performance is highly dependent on hardware, traffic profile, and overall system load. I provide the statistics here just to give a general idea of the improvement brought by OVS 3.0.0.&lt;/p&gt; &lt;h3 id="traffic-generator-calibration"&gt;Baseline results for comparison&lt;/h3&gt; &lt;p&gt;For reference, the tests were executed with a cable connecting &lt;code&gt;port0&lt;/code&gt; and &lt;code&gt;port1&lt;/code&gt; of the traffic generator machine. This is the maximum performance TRex is able to achieve with this configuration and hardware.&lt;/p&gt; &lt;table&gt;&lt;caption&gt;Table 1: Maximum traffic generator performance.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type&lt;/th&gt; &lt;th&gt;Connection rate&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Short-lived&lt;/td&gt; &lt;td&gt;1.8M conn/s&lt;/td&gt; &lt;td&gt;1.7M&lt;/td&gt; &lt;td&gt;8.4G bit/s&lt;/td&gt; &lt;td&gt;12.7M pkt/s&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Long-lived&lt;/td&gt; &lt;td&gt;11.1K conn/s&lt;/td&gt; &lt;td&gt;898K&lt;/td&gt; &lt;td&gt;8.0G bit/s&lt;/td&gt; &lt;td&gt;11.4M pkt/s&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="1-cpu-1-queue-per-port-without-connection-tracking"&gt;1 CPU, 1 queue per port, without connection tracking&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flow br0 action=normal &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 2: Short-lived connections with 1 CPU, 1 queue per port, without connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;1.0M conn/s&lt;/td&gt; &lt;td&gt;524.8K&lt;/td&gt; &lt;td&gt;4.5G bit/s&lt;/td&gt; &lt;td&gt;7.3M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.0M conn/s&lt;/td&gt; &lt;td&gt;513.1K&lt;/td&gt; &lt;td&gt;4.5G bit/s&lt;/td&gt; &lt;td&gt;7.1M pkt/s&lt;/td&gt; &lt;td&gt;-1.74%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 3: Long-lived connections with 1 CPU, 1 queue per port, without connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;3.1K conn/s&lt;/td&gt; &lt;td&gt;79.9K&lt;/td&gt; &lt;td&gt;3.1G bit/s&lt;/td&gt; &lt;td&gt;4.7M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.8K conn/s&lt;/td&gt; &lt;td&gt;71.9K&lt;/td&gt; &lt;td&gt;2.8G bit/s&lt;/td&gt; &lt;td&gt;4.2M pkt/s&lt;/td&gt; &lt;td&gt;-9.82%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;There is a drop in performance, without connection tracking enabled, between versions 2.17.2 and 3.0.0. This drop is completely unrelated to the conntrack optimization patch series I am focusing on. It might be caused by some discrepancies in the test procedure, but it might also have been introduced by another patch series between the two tested versions.&lt;/p&gt; &lt;h3 id="1-cpu-1-queue-per-port"&gt;1 CPU, 1 queue per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 4: Short-lived connections with 1 CPU, 1 queue per port, with connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;39.7K conn/s&lt;/td&gt; &lt;td&gt;20.0K&lt;/td&gt; &lt;td&gt;172.0M bit/s&lt;/td&gt; &lt;td&gt;275.8K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;48.2K conn/s&lt;/td&gt; &lt;td&gt;24.3K&lt;/td&gt; &lt;td&gt;208.9M bit/s&lt;/td&gt; &lt;td&gt;334.9K pkt/s&lt;/td&gt; &lt;td&gt;+21.36%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 5: Long-lived connections with 1 CPU, 1 queue per port, with connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;959 conn/s&lt;/td&gt; &lt;td&gt;24.7K&lt;/td&gt; &lt;td&gt;956.6M bit/s&lt;/td&gt; &lt;td&gt;1.4M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.2K conn/s&lt;/td&gt; &lt;td&gt;31.5K&lt;/td&gt; &lt;td&gt;1.2G bit/s&lt;/td&gt; &lt;td&gt;1.8M pkt/s&lt;/td&gt; &lt;td&gt;+28.15%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Already here, we can see that the patch series improves the single-threaded performance of connection tracking during the creation, destruction, and lookup code paths. Keep these results in mind when looking at improvements in multithreaded performance.&lt;/p&gt; &lt;h3 id="2-cpus-1-queue-per-port"&gt;2 CPUs, 1 queue per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x2002" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 6: Short-lived connections with 2 CPUs, 1 queue per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;39.9K conn/s&lt;/td&gt; &lt;td&gt;20.0K&lt;/td&gt; &lt;td&gt;172.8M bit/s&lt;/td&gt; &lt;td&gt;277.0K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;46.8K conn/s&lt;/td&gt; &lt;td&gt;23.5K&lt;/td&gt; &lt;td&gt;202.7M bit/s&lt;/td&gt; &lt;td&gt;325.0K pkt/s&lt;/td&gt; &lt;td&gt;+17.28%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 7: Long-lived connections with 2 CPUs, 1 queue per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;885 conn/s&lt;/td&gt; &lt;td&gt;22.7K&lt;/td&gt; &lt;td&gt;883.1M bit/s&lt;/td&gt; &lt;td&gt;1.3M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.1K conn/s&lt;/td&gt; &lt;td&gt;28.6K&lt;/td&gt; &lt;td&gt;1.1G bit/s&lt;/td&gt; &lt;td&gt;1.7M pkt/s&lt;/td&gt; &lt;td&gt;+25.19%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;It is worth noting that assigning twice as many CPUs to do packet processing does not double the performance. Far from it, in fact. The numbers are exactly the same (if not lower) than with only one CPU.&lt;/p&gt; &lt;p&gt;This surprising result might be caused because there is only one RX queue per port and each CPU processes a single port.&lt;/p&gt; &lt;h3 id="2-cpus-2-queues-per-port"&gt;2 CPUs, 2 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x2002" ovs-vsctl set Interface port0 options:n_rxq=2 ovs-vsctl set Interface port1 options:n_rxq=2 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 8: Short-lived connections with 2 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;48.3K conn/s&lt;/td&gt; &lt;td&gt;24.3K&lt;/td&gt; &lt;td&gt;208.8M bit/s&lt;/td&gt; &lt;td&gt;334.8K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;65.9K conn/s&lt;/td&gt; &lt;td&gt;33.2K&lt;/td&gt; &lt;td&gt;286.8M bit/s&lt;/td&gt; &lt;td&gt;459.9K pkt/s&lt;/td&gt; &lt;td&gt;+36.41%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For short-lived connections, we begin to see improvement beyond the single-threaded performance gain. Lock contention was reduced during the insertion and deletion of conntrack entries.&lt;/p&gt; &lt;table&gt;&lt;caption&gt; &lt;p&gt;Table 9: Long-lived connections with 2 CPUs, 2 queues per port.&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;1.1K conn/s&lt;/td&gt; &lt;td&gt;29.1K&lt;/td&gt; &lt;td&gt;1.1G bit/s&lt;/td&gt; &lt;td&gt;1.7M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.4K conn/s&lt;/td&gt; &lt;td&gt;37.0K&lt;/td&gt; &lt;td&gt;1.4G bit/s&lt;/td&gt; &lt;td&gt;2.2M pkt/s&lt;/td&gt; &lt;td&gt;+26.77%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;With two CPUs and two queues, if we take the single-threaded performance out of the picture, there seems to be no improvement in conntrack lookup for long-lived connections.&lt;/p&gt; &lt;h3 id="4-cpus-2-queues-per-port"&gt;4 CPUs, 2 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x6006" ovs-vsctl set Interface port0 options:n_rxq=2 ovs-vsctl set Interface port1 options:n_rxq=2 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 10: Short-lived connections with 4 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;47.4K conn/s&lt;/td&gt; &lt;td&gt;23.9K&lt;/td&gt; &lt;td&gt;206.2M bit/s&lt;/td&gt; &lt;td&gt;330.6K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;49.1K conn/s&lt;/td&gt; &lt;td&gt;24.7K&lt;/td&gt; &lt;td&gt;212.1M bit/s&lt;/td&gt; &lt;td&gt;340.1K pkt/s&lt;/td&gt; &lt;td&gt; &lt;p&gt;+3.53%&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The short-lived connection rate performance has dropped in 3.0.0. This is not a fluke: The numbers are consistent across multiple runs. This drop warrants some scrutiny, but does not invalidate all the work that has been done.&lt;/p&gt; &lt;table&gt;&lt;caption&gt;Table 11: Long-lived connections with 4 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;981 conn/s&lt;/td&gt; &lt;td&gt;25.2K&lt;/td&gt; &lt;td&gt;977.7M bit/s&lt;/td&gt; &lt;td&gt;1.5M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.0K conn/s&lt;/td&gt; &lt;td&gt;52.4K&lt;/td&gt; &lt;td&gt;2.0G bit/s&lt;/td&gt; &lt;td&gt;3.1M pkt/s&lt;/td&gt; &lt;td&gt;+108.31%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;With four CPUs and two queues per port, long-lived connections tracking is starting to scale up.&lt;/p&gt; &lt;h3 id="4-cpus-4-queues-per-port"&gt;4 CPUs, 4 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x6006" ovs-vsctl set Interface port0 options:n_rxq=4 ovs-vsctl set Interface port1 options:n_rxq=4 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 12: Short-lived connections with 4 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;66.1K conn/s&lt;/td&gt; &lt;td&gt;33.2K&lt;/td&gt; &lt;td&gt;286.4M bit/s&lt;/td&gt; &lt;td&gt;459.2K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;100.8K conn/s&lt;/td&gt; &lt;td&gt;50.6K&lt;/td&gt; &lt;td&gt;437.0M bit/s&lt;/td&gt; &lt;td&gt;700.6K pkt/s&lt;/td&gt; &lt;td&gt;+52.55%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 13: Long-lived connections with 4 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;996 conn/s&lt;/td&gt; &lt;td&gt;25.9K&lt;/td&gt; &lt;td&gt;994.2M bit/s&lt;/td&gt; &lt;td&gt;1.5M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.6K conn/s&lt;/td&gt; &lt;td&gt;67.0K&lt;/td&gt; &lt;td&gt;2.6G bit/s&lt;/td&gt; &lt;td&gt;3.9M pkt/s&lt;/td&gt; &lt;td&gt;+162.89%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="8-cpus-4-queues-per-port"&gt;8 CPUs, 4 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x1e01e" ovs-vsctl set Interface port0 options:n_rxq=4 ovs-vsctl set Interface port1 options:n_rxq=4 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 14: Short-lived connections with 8 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;62.2K conn/s&lt;/td&gt; &lt;td&gt;31.3K&lt;/td&gt; &lt;td&gt;269.8M bit/s&lt;/td&gt; &lt;td&gt;432.5K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;90.1K conn/s&lt;/td&gt; &lt;td&gt;45.2K&lt;/td&gt; &lt;td&gt;390.9M bit/s&lt;/td&gt; &lt;td&gt;626.7K pkt/s&lt;/td&gt; &lt;td&gt;+44.89%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 15: Long-lived connections with 8 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;576 conn/s&lt;/td&gt; &lt;td&gt;17.1K&lt;/td&gt; &lt;td&gt;567.2M bit/s&lt;/td&gt; &lt;td&gt;852.5K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;3.8K conn/s&lt;/td&gt; &lt;td&gt;97.8K&lt;/td&gt; &lt;td&gt;3.8G bit/s&lt;/td&gt; &lt;td&gt;5.7M pkt/s&lt;/td&gt; &lt;td&gt;+562.76%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="8-cpus-8-queues-per-port"&gt;8 CPUs, 8 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x1e01e" ovs-vsctl set Interface port0 options:n_rxq=8 ovs-vsctl set Interface port1 options:n_rxq=8 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 16: Short-lived connections with 8 CPUs, 8 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;50.6K conn/s&lt;/td&gt; &lt;td&gt;25.5K&lt;/td&gt; &lt;td&gt;219.5M bit/s&lt;/td&gt; &lt;td&gt;351.9K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;100.9K conn/s&lt;/td&gt; &lt;td&gt;50.7K&lt;/td&gt; &lt;td&gt;436.0M bit/s&lt;/td&gt; &lt;td&gt;698.9K pkt/s&lt;/td&gt; &lt;td&gt;+99.36%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 17: Long-lived connections with 8 CPUs, 8 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;541 conn/s&lt;/td&gt; &lt;td&gt;14.0K&lt;/td&gt; &lt;td&gt;539.2M bit/s&lt;/td&gt; &lt;td&gt;810.3K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;4.8K conn/s&lt;/td&gt; &lt;td&gt;124.1K&lt;/td&gt; &lt;td&gt;4.8G bit/s&lt;/td&gt; &lt;td&gt;7.2M pkt/s&lt;/td&gt; &lt;td&gt;+792.83%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id="analysis"&gt;Performance improvements in version 3.0.0 of Open vSwitch&lt;/h2&gt; &lt;p&gt;Using the tools in this article, I have been able to record advances made in version 3.0.0 in scaling and in handling long-lived connections.&lt;/p&gt; &lt;h3 id="scaling"&gt;Scaling&lt;/h3&gt; &lt;p&gt;Figure 2 shows how many insertions and deletions per second were achieved on different system configurations for short-lived connections.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_13.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_13.png?itok=x4T-r8hk" width="600" height="372" alt="Chart showing improvements in scaling of short-lived connections tracking in version 3.0.0" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Improvements in scaling of short-lived connections tracking in version 3.0.0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Apart from the small blip with 4 CPUs and 2 queues per port, the conntrack insertion and deletion code path has improved consistently in OvS 3.0.0. The multithreaded lock contention remains, but is less noticeable than with OvS 2.17.2.&lt;/p&gt; &lt;p&gt;Figure 3 shows how many insertions and deletions per second were achieved on different system configurations for long-lived connections.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_6.png?itok=F-iPPYTf" width="600" height="369" alt="Chart showing improvements in scaling of long-lived connections tracking in version 3.0.0." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Improvements in scaling of long-lived connections tracking in version 3.0.0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Long-lived connections tracking is where the optimizations done in OvS 3.0.0 really shine. The reduction in multithreaded lock contention with conntrack lookup makes the performance scale significantly better with the number of CPUs.&lt;/p&gt; &lt;h3 id="profiling"&gt;Performance during high traffic&lt;/h3&gt; &lt;p&gt;The following commands generate profiling reports using the Linux kernel's &lt;a href="http://perf.wiki.kernel.org/"&gt;perf command&lt;/a&gt;. I measured the performance of both version 2.17.2 and version 3.0.0 for 8 CPUs and 8 RX queues under a maximum load for long-lived connections, with conntrack flows enabled. Only the events of a single CPU were captured:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;perf record -g -C 1 sleep 60 perf report -U --no-children | grep '\[[\.k]\]' | head -15 &gt; profile-$version.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the subsections that follow, I have manually annotated lines that are directly related to acquiring mutexes so that they start with a &lt;code&gt;*&lt;/code&gt; character. When a CPU is waiting for a mutex acquisition, it is not processing any network traffic, but waiting for another CPU to release the lock.&lt;/p&gt; &lt;h4 id="2172"&gt;Performance in version 2.17.2&lt;/h4&gt; &lt;p&gt;The profiled CPU spends almost 40% of its cycles acquiring locks and waiting for other CPUs to release locks:&lt;/p&gt; &lt;pre&gt; * 30.99% pmd-c01/id:5 libc.so.6 [.] pthread_mutex_lock@@GLIBC_2.2.5 12.27% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_process_rxq_port 5.18% pmd-c01/id:5 ovs-vswitchd [.] netdev_dpdk_rxq_recv 4.24% pmd-c01/id:5 ovs-vswitchd [.] pmd_thread_main 3.93% pmd-c01/id:5 ovs-vswitchd [.] pmd_perf_end_iteration * 3.63% pmd-c01/id:5 libc.so.6 [.] __GI___pthread_mutex_unlock_usercnt 3.62% pmd-c01/id:5 ovs-vswitchd [.] i40e_recv_pkts_vec_avx2 * 2.76% pmd-c01/id:5 [kernel.kallsyms] [k] syscall_exit_to_user_mode * 0.91% pmd-c01/id:5 libc.so.6 [.] __GI___lll_lock_wait * 0.18% pmd-c01/id:5 [kernel.kallsyms] [k] __x64_sys_futex * 0.17% pmd-c01/id:5 [kernel.kallsyms] [k] futex_wait * 0.12% pmd-c01/id:5 [kernel.kallsyms] [k] entry_SYSCALL_64_after_hwframe * 0.11% pmd-c01/id:5 libc.so.6 [.] __GI___lll_lock_wake * 0.08% pmd-c01/id:5 [kernel.kallsyms] [k] do_syscall_64 * 0.06% pmd-c01/id:5 [kernel.kallsyms] [k] do_futex&lt;/pre&gt; &lt;h4 id="300"&gt;Performance in version 3.0.0&lt;/h4&gt; &lt;p&gt;It is obvious that 3.0.0 has much less lock contention and therefore scales better with the number of CPUs:&lt;/p&gt; &lt;pre&gt; 15.30% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_input__ 8.62% pmd-c01/id:5 ovs-vswitchd [.] conn_key_lookup 7.88% pmd-c01/id:5 ovs-vswitchd [.] miniflow_extract 7.75% pmd-c01/id:5 ovs-vswitchd [.] cmap_find * 6.92% pmd-c01/id:5 libc.so.6 [.] pthread_mutex_lock@@GLIBC_2.2.5 5.15% pmd-c01/id:5 ovs-vswitchd [.] dpcls_subtable_lookup_mf_u0w4_u1w1 4.16% pmd-c01/id:5 ovs-vswitchd [.] cmap_find_batch 4.10% pmd-c01/id:5 ovs-vswitchd [.] tcp_conn_update 3.86% pmd-c01/id:5 ovs-vswitchd [.] dpcls_subtable_lookup_mf_u0w5_u1w1 3.51% pmd-c01/id:5 ovs-vswitchd [.] conntrack_execute 3.42% pmd-c01/id:5 ovs-vswitchd [.] i40e_xmit_fixed_burst_vec_avx2 0.77% pmd-c01/id:5 ovs-vswitchd [.] dp_execute_cb 0.72% pmd-c01/id:5 ovs-vswitchd [.] netdev_dpdk_rxq_recv 0.07% pmd-c01/id:5 ovs-vswitchd [.] i40e_xmit_pkts_vec_avx2 0.04% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_input &lt;/pre&gt; &lt;h2 id="final-words"&gt;Final words&lt;/h2&gt; &lt;p&gt;I hope this gave you some ideas for benchmarking and profiling connection tracking with TRex and &lt;code&gt;perf&lt;/code&gt;. Please leave any questions you have in comments on this article.&lt;/p&gt; &lt;p&gt;Kudos to Paolo Valerio and Gaëtan Rivet for their work on optimizing the user space OvS conntrack implementation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300" title="Benchmarking improved conntrack performance in OvS 3.0.0"&gt;Benchmarking improved conntrack performance in OvS 3.0.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Robin Jarry</dc:creator><dc:date>2022-11-17T07:00:00Z</dc:date></entry><entry><title>Our (bumpy) road to Jakarta EE 10</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/our-bumpy-road-to-jakarta-ee-10/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/our-bumpy-road-to-jakarta-ee-10/</id><updated>2022-11-17T00:00:00Z</updated><published>2022-11-17T00:00:00Z</published><summary type="html">Quarkus has been relatively silent on the Jakarta EE front until a few weeks ago, compared to some other frameworks who announced early clear plans with timelines. That doesn’t mean we were not actively preparing the transition and we have been incredibly busy making it a reality. Most of you...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-11-17T00:00:00Z</dc:date></entry></feed>
